<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Big data,Spark,Hadoop,Spark Job-server," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="I. Version and Environment1. Spark, Yarn, HDFS[Spark and Hadoop Downloads](http://spark.apache.org/downloads.html)  * Current: spark-2.2.0-bin-hadoop2.7.tgz * Latest: spark-2.3.0-bin-hadoop2.7.tgz 2.">
<meta name="keywords" content="Big data,Spark,Hadoop,Spark Job-server">
<meta property="og:type" content="article">
<meta property="og:title" content="Productive Spark Cluster in YARN-Client Mode">
<meta property="og:url" content="http://cubeanliu.com/2018/02/20/ProductiveSparkCluster/index.html">
<meta property="og:site_name" content="Cubean Blog">
<meta property="og:description" content="I. Version and Environment1. Spark, Yarn, HDFS[Spark and Hadoop Downloads](http://spark.apache.org/downloads.html)  * Current: spark-2.2.0-bin-hadoop2.7.tgz * Latest: spark-2.3.0-bin-hadoop2.7.tgz 2.">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/images/spark-ports.gif">
<meta property="og:updated_time" content="2018-05-09T10:49:52.454Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Productive Spark Cluster in YARN-Client Mode">
<meta name="twitter:description" content="I. Version and Environment1. Spark, Yarn, HDFS[Spark and Hadoop Downloads](http://spark.apache.org/downloads.html)  * Current: spark-2.2.0-bin-hadoop2.7.tgz * Latest: spark-2.3.0-bin-hadoop2.7.tgz 2.">
<meta name="twitter:image" content="https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/images/spark-ports.gif">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://cubeanliu.com/2018/02/20/ProductiveSparkCluster/"/>





  <title>Productive Spark Cluster in YARN-Client Mode | Cubean Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '[object Object]', 'auto');
  ga('send', 'pageview');
</script>











</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Cubean Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">Change the world with AI and IoT</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://cubeanliu.com/2018/02/20/ProductiveSparkCluster/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cubean Liu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Cubean Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Productive Spark Cluster in YARN-Client Mode</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-20T12:41:07+11:00">
                2018-02-20
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/20/ProductiveSparkCluster/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/02/20/ProductiveSparkCluster/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/02/20/ProductiveSparkCluster/" class="leancloud_visitors" data-flag-title="Productive Spark Cluster in YARN-Client Mode">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="I-Version-and-Environment"><a href="#I-Version-and-Environment" class="headerlink" title="I. Version and Environment"></a>I. Version and Environment</h2><h3 id="1-Spark-Yarn-HDFS"><a href="#1-Spark-Yarn-HDFS" class="headerlink" title="1. Spark, Yarn, HDFS"></a>1. Spark, Yarn, HDFS</h3><pre><code>[Spark and Hadoop Downloads](http://spark.apache.org/downloads.html)

* Current: spark-2.2.0-bin-hadoop2.7.tgz
* Latest: spark-2.3.0-bin-hadoop2.7.tgz
</code></pre><h3 id="2-Spark-Job-Server"><a href="#2-Spark-Job-Server" class="headerlink" title="2. Spark Job Server"></a>2. Spark Job Server</h3><p>  <a href="https://github.com/spark-jobserver/spark-jobserver" target="_blank" rel="noopener">spark-jobserver github</a></p>
<pre><code>### Understand Client and Cluster ModePermalink

Spark jobs can run on YARN in two modes: cluster mode and client mode. Understanding the difference between the two modes is important for choosing an appropriate memory allocation configuration, and to submit jobs as expected.

A Spark job consists of two parts: Spark Executors that run the actual tasks, and a Spark Driver that schedules the Executors.

- Cluster mode: everything runs inside the cluster. You can start a job from your laptop and the job will continue running even if you close your computer. In this mode, the Spark Driver is encapsulated inside the YARN Application Master.

- Client mode: the Spark driver runs on a client, such as your laptop. If the client is shut down, the job fails. Spark Executors still run on the cluster, and to schedule everything, a small YARN Application Master is created.

Client mode is well suited for interactive jobs, but applications will fail if the client stops. For long running jobs, cluster mode is more appropriate.

[Configuring Job Server for YARN in client mode with docker](https://github.com/spark-jobserver/spark-jobserver/blob/master/doc/yarn.md)
</code></pre><p>fxs-draganddrop<br>    <a href="https://github.com/spark-jobserver/spark-jobserver/blob/master/doc/cluster.md" target="_blank" rel="noopener">Configuring Job Server for YARN cluster mode</a></p>
<a id="more"></a>
<h3 id="3-OS-in-AWS"><a href="#3-OS-in-AWS" class="headerlink" title="3. OS in AWS"></a>3. OS in AWS</h3><ul>
<li>OS: RHEL-7.2_HVM-20161025-x86_64-1-Hourly2-GP2 - ami-91cdf0f2</li>
<li>Disk: 200 GB in root and 1 TB in additional disk /dev/xvdb</li>
</ul>
<p><em>Common Steps</em>:</p>
<ul>
<li><p>Setup timezone</p>
  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo timedatectl <span class="built_in">set</span>-timezone Australia/Sydney</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Reference-Installation"><a href="#Reference-Installation" class="headerlink" title="Reference Installation"></a>Reference Installation</h3><pre><code>[Install, Configure, and Run Spark on Top of a Hadoop YARN Cluster](https://linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/)
</code></pre><h2 id="II-Building-YARN-HDFS-cluster-servers"><a href="#II-Building-YARN-HDFS-cluster-servers" class="headerlink" title="II. Building YARN-HDFS cluster servers"></a>II. Building YARN-HDFS cluster servers</h2><p>Setting up the common environment in both NameNode and DataNodes.</p>
<h3 id="1-Mount-2nd-EBS-disk-to-HDFS-folder"><a href="#1-Mount-2nd-EBS-disk-to-HDFS-folder" class="headerlink" title="1. Mount 2nd EBS disk to HDFS folder"></a>1. Mount 2nd EBS disk to HDFS folder</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo fdisk -l</span><br><span class="line">$ sudo mkfs.ext4 /dev/xvdb</span><br><span class="line">$ sudo mkdir /hadoop</span><br><span class="line">$ sudo vi /etc/fstab</span><br><span class="line">/dev/xvdb      /hadoop         ext4    defaults,nofail 0 2</span><br><span class="line"></span><br><span class="line">$ sudo mount -a</span><br><span class="line">$ df -h</span><br><span class="line"></span><br><span class="line">$ sudo rm -rf /hadoop/*</span><br><span class="line">$ sudo mkdir -p /hadoop/dfs/data</span><br></pre></td></tr></table></figure>
<h3 id="2-Install-Hadoop"><a href="#2-Install-Hadoop" class="headerlink" title="2. Install Hadoop"></a>2. Install Hadoop</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">curl -O http://apache.mirrors.ionfish.org/hadoop/common/hadoop-2.7.5/hadoop-2.7.5.tar.gz</span><br><span class="line"></span><br><span class="line">tar xvf hadoop-2.7.5.tar.gz</span><br><span class="line">ln -sfn hadoop-2.7.5 hadoop</span><br><span class="line"></span><br><span class="line">// unlink hadoop (will release the symbol link)</span><br></pre></td></tr></table></figure>
<h3 id="3-bash-environment-changes"><a href="#3-bash-environment-changes" class="headerlink" title="3. bash environment changes"></a>3. bash environment changes</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># vi ~/.bash_profile ( ~/.bashrc file as well )</span><br><span class="line">	</span><br><span class="line">export JAVA_HOME=/usr/java/default</span><br><span class="line">export HADOOP_HOME=/opt/hadoop</span><br><span class="line">export HADOOP_INSTALL=$HADOOP_HOME</span><br><span class="line">export HADOOP_MAPRED_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_HDFS_HOME=$HADOOP_HOME</span><br><span class="line">export YARN_HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME</span><br><span class="line">export HADOOP_PREFIX=$HADOOP_HOME</span><br><span class="line">export HADOOP_LIBEXEC_DIR=$HADOOP_HOME/libexec</span><br><span class="line">export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</span><br></pre></td></tr></table></figure>
<h3 id="4-Configuring-Hadoop"><a href="#4-Configuring-Hadoop" class="headerlink" title="4. Configuring Hadoop"></a>4. Configuring Hadoop</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat /opt/hadoop/etc/hadoop/core-site.xml</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;fs.default.name&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;hdfs://&lt;HDFS Namenode IP&gt;:9000&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">   		&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;/hadoop/tmp&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cat /opt/hadoop/etc/hadoop/hdfs-site.xml</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;file:///hadoop/dfs/name&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;dfs.data.dir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;file:///hadoop/dfs/data&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat /opt/hadoop/etc/hadoop/mapred-site.xml</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat /opt/hadoop/etc/hadoop/yarn-site.xml</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;NameNodeIP&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /opt/hadoop/etc/hadoop/slaves</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Reference settings</p>
</blockquote>
<p>hadoop.tmp.dir (A base for other temporary directories) is property, that need to be set in core-site.xml, it is like export in linux</p>
<p>Ex:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/name&lt;/value&gt;</span><br></pre></td></tr></table></figure>
<p>You can use reference of hadoop.tmp.dir in hdfs-site.xml like above</p>
<p>For more core-site.xml and hdfs-site.xml</p>
<p>There’re three HDFS properties which contain hadoop.tmp.dir in their values</p>
<ul>
<li><p>dfs.name.dir: directory where namenode stores its metadata, with default value ${hadoop.tmp.dir}/dfs/name.</p>
</li>
<li><p>dfs.data.dir: directory where HDFS data blocks are stored, with default value ${hadoop.tmp.dir}/dfs/data.</p>
</li>
<li><p>fs.checkpoint.dir: directory where secondary namenode store its checkpoints, default value is ${hadoop.tmp.dir}/dfs/namesecondary</p>
</li>
</ul>
<h3 id="5-Running-Hadoop-YARN"><a href="#5-Running-Hadoop-YARN" class="headerlink" title="5. Running Hadoop-YARN"></a>5. Running Hadoop-YARN</h3><p>In namenode server</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Format NameNode</span></span><br><span class="line">/opt/hadoop/bin/hadoop namenode -format</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/hadoop/sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/hadoop/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>To check if hadoop dfs is up and running go to http://<namenode ip="">:50070/dfshealth.html</namenode></p>
<h2 id="III-Building-Spark-cluster-servers"><a href="#III-Building-Spark-cluster-servers" class="headerlink" title="III. Building Spark cluster servers"></a>III. Building Spark cluster servers</h2><h3 id="1-In-all-nodes-need-to-add-domain-names-in-etc-hosts"><a href="#1-In-all-nodes-need-to-add-domain-names-in-etc-hosts" class="headerlink" title="1. In all nodes, need to add domain names in /etc/hosts"></a>1. In all nodes, need to add domain names in /etc/hosts</h3><h3 id="2-Setup-master-node"><a href="#2-Setup-master-node" class="headerlink" title="2. Setup master node"></a>2. Setup master node</h3><p>On master server:</p>
<ul>
<li>Install Spark</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">curl -O http://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz</span><br><span class="line">tar xvf spark-2.2.0-bin-hadoop2.7.tgz</span><br><span class="line">ln -sfn spark-2.2.0-bin-hadoop2.7 spark</span><br></pre></td></tr></table></figure>
<ul>
<li>change spark-defaults.conf</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cp /opt/spark/conf/spark-defaults.conf.template /opt/spark/conf/spark-defaults.conf</span><br><span class="line">$ vim /opt/spark/conf/spark-defaults.conf </span><br><span class="line"> spark.master                     spark://&lt;Master IP&gt;:7077</span><br><span class="line"> spark.eventLog.enabled           <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Setup slave nodes</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /opt/spark/conf/slaves</span><br><span class="line"></span><br><span class="line"><span class="comment"># A Spark Worker will be started on each of the machines listed below.</span></span><br><span class="line">&lt;Slave IPs&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>Copy all to slave nodes</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/hadoop /opt/spark &lt;Slave IP&gt;:/opt/</span><br></pre></td></tr></table></figure>
<h3 id="3-Start-Spark-cluster"><a href="#3-Start-Spark-cluster" class="headerlink" title="3. Start Spark cluster"></a>3. Start Spark cluster</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/spark/sbin/start-all.sh</span><br></pre></td></tr></table></figure>
<p>To check if Spark Cluster is up and running http://<master ip="">:8080/</master></p>
<h2 id="IV-Install-Docker"><a href="#IV-Install-Docker" class="headerlink" title="IV. Install Docker"></a>IV. Install Docker</h2><h3 id="1-Mount-2nd-EBS-disk-to-docker-folder"><a href="#1-Mount-2nd-EBS-disk-to-docker-folder" class="headerlink" title="1. Mount 2nd EBS disk to docker folder:"></a>1. Mount 2nd EBS disk to docker folder:</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo fdisk -l</span><br><span class="line">$ sudo mkfs.ext4 /dev/xvdb</span><br><span class="line">$ sudo mkdir -p /var/lib/docker</span><br><span class="line">$ sudo vi /etc/fstab</span><br><span class="line">/dev/xvdb      /var/lib/docker         ext4    defaults,nofail 0 2</span><br><span class="line"></span><br><span class="line">$ sudo mount -a</span><br><span class="line">$ df -h</span><br><span class="line"></span><br><span class="line">$ sudo rm -rf /var/lib/docker/*</span><br></pre></td></tr></table></figure>
<h3 id="2-Install-Docker-community-version"><a href="#2-Install-Docker-community-version" class="headerlink" title="2. Install Docker community version"></a>2. Install Docker community version</h3><p>Then install docker-ce, docker-py(required by docker login)</p>
<p>In AWS linux (RHEL):</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install docker</span><br><span class="line">sudo service docker start</span><br><span class="line">sudo service docker status</span><br><span class="line"></span><br><span class="line">sudo usermode -aG docker ec2-user</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> spark-jobserver</span><br><span class="line">sbt docker</span><br></pre></td></tr></table></figure>
<h3 id="3-Optional-Install-Ansible"><a href="#3-Optional-Install-Ansible" class="headerlink" title="3. [Optional] Install Ansible"></a>3. [Optional] Install Ansible</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">$ sudo yum -y localinstall epel-release-latest-7.noarch.rpm</span><br><span class="line">$ sudo yum -y install python-pip</span><br><span class="line">$ sudo pip install --upgrade pip</span><br><span class="line"></span><br><span class="line">$ sudo pip install ansible</span><br><span class="line"></span><br><span class="line">$ sudo pip install datadog</span><br><span class="line">$ sudo yum -y install git</span><br></pre></td></tr></table></figure>
<h2 id="V-Run-spark-job-server"><a href="#V-Run-spark-job-server" class="headerlink" title="V. Run spark-job-server"></a>V. Run spark-job-server</h2><h3 id="1-Spark-job-server-docker-config"><a href="#1-Spark-job-server-docker-config" class="headerlink" title="1. Spark job-server docker config"></a>1. Spark job-server docker config</h3><p>spark-jobserver/job-server/config/docker.conf</p>
<p>Default, using spark local mode.<br>In prod, we are using yarn-client mode. Give parameters thought job-server interface.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Template for Spark Job Server Docker config</span></span><br><span class="line"><span class="comment"># You can easily override the spark master through SPARK_MASTER env variable</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Spark Cluster / Job Server configuration</span></span><br><span class="line">spark &#123;</span><br><span class="line">  <span class="comment"># spark.master will be passed to each job's JobContext</span></span><br><span class="line">  <span class="comment"># local[...], yarn, mesos://... or spark://...</span></span><br><span class="line">  master = <span class="string">"local[*]"</span></span><br><span class="line">  master = <span class="variable">$&#123;?SPARK_MASTER&#125;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># client or cluster deployment</span></span><br><span class="line">  submit.deployMode = <span class="string">"client"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Default # of CPUs for jobs to use for Spark standalone cluster</span></span><br><span class="line">  job-number-cpus = 8</span><br><span class="line"></span><br><span class="line">  jobserver &#123;</span><br><span class="line">    port = 8090</span><br><span class="line">    jobdao = spark.jobserver.io.JobSqlDAO</span><br><span class="line"></span><br><span class="line">    context-per-jvm = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">    // <span class="keyword">for</span> client calling timeout. Cubean added at Sat 28 Oct 2017</span><br><span class="line">    short-timeout = 600s</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Default client mode will start up a new JobManager int local machine</span></span><br><span class="line">    <span class="comment"># You can use mesos-cluster mode with REMOTE_JOBSERVER_DIR and MESOS_SPARK_DISPATCHER</span></span><br><span class="line">    <span class="comment"># environment value set in xxxx.sh file to launch JobManager in remote node</span></span><br><span class="line">    <span class="comment"># Mesos will take responsibility to offer resource to the JobManager process</span></span><br><span class="line">    driver-mode = client</span><br><span class="line"></span><br><span class="line">    sqldao &#123;</span><br><span class="line">      <span class="comment"># Directory where default H2 driver stores its data. Only needed for H2.</span></span><br><span class="line">      rootdir = /database</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Full JDBC URL / init string.  Sorry, needs to match above.</span></span><br><span class="line">      <span class="comment"># Substitutions may be used to launch job-server, but leave it out here in the default or tests won't pass</span></span><br><span class="line">      jdbc.url = <span class="string">"jdbc:h2:file:/database/h2-db"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment"># predefined Spark contexts</span></span><br><span class="line">  <span class="comment"># contexts &#123;</span></span><br><span class="line">  <span class="comment">#   my-low-latency-context &#123;</span></span><br><span class="line">  <span class="comment">#     num-cpu-cores = 1           # Number of cores to allocate.  Required.</span></span><br><span class="line">  <span class="comment">#     memory-per-node = 512m         # Executor memory per node, -Xmx style eg 512m, 1G, etc.</span></span><br><span class="line">  <span class="comment">#   &#125;</span></span><br><span class="line">  <span class="comment">#   # define additional contexts here</span></span><br><span class="line">  <span class="comment"># &#125;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># universal context configuration.  These settings can be overridden, see README.md</span></span><br><span class="line">  context-settings &#123;</span><br><span class="line">    num-cpu-cores = 15           <span class="comment"># Number of cores to allocate.  Required.</span></span><br><span class="line">    memory-per-node = 40G         <span class="comment"># Executor memory per node, -Xmx style eg 512m, #1G, etc.</span></span><br><span class="line"></span><br><span class="line">    spark.ui.enabled = <span class="literal">true</span></span><br><span class="line">    spark.ui.port = 4040</span><br><span class="line"></span><br><span class="line">    <span class="comment">#spark.yarn.preserve.staging.files = true</span></span><br><span class="line">    <span class="comment">#spark.serializer = org.apache.spark.serializer.KryoSerializer</span></span><br><span class="line">    <span class="comment">#spark.sql.parquet.output.committer.class = org.apache.spark.sql.parquet.DirectParquetOutputCommitter</span></span><br><span class="line">    <span class="comment">#spark.sql.parquet.compression.codec = snappy</span></span><br><span class="line">    <span class="comment">#spark.scheduler.mode = FAIR</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#spark.driver.extraClassPath = "/etc/hadoop/conf:/etc/hive/conf:/usr/lib/hadoop/*:/usr/lib/hadoop-hdfs/*:/usr/lib/hadoop-yarn/*:/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*"</span></span><br><span class="line">    <span class="comment">#spark.driver.extraLibraryPath = "/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native"</span></span><br><span class="line">    <span class="comment">#spark.executor.extraClassPath = "/etc/hadoop/conf:/etc/hive/conf:/usr/lib/hadoop/*:/usr/lib/hadoop-hdfs/*:/usr/lib/hadoop-yarn/*:/usr/lib/hadoop-lzo/lib/*:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*"</span></span><br><span class="line">    <span class="comment">#spark.executor.extraLibraryPath = "/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># spark.eventLog.enabled = true</span></span><br><span class="line">    <span class="comment"># spark.eventLog.dir = "hdfs://&lt;interal ip&gt;:9000/tmp/spark-events"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># It is still possible to construct the UI of an application through Spark’s history server,</span></span><br><span class="line">    <span class="comment"># provided that the application’s event logs exist.</span></span><br><span class="line">    <span class="comment"># You can start the history server by executing:</span></span><br><span class="line">    <span class="comment"># ./sbin/start-history-server.sh</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># spark.history.fs.logDirectory = "hdfs://&lt;interal ip&gt;:9000/tmp/spark-events"</span></span><br><span class="line">    <span class="comment"># spark.yarn.historyServer.address = "&lt;interal ip&gt;:18080"</span></span><br><span class="line">    <span class="comment"># spark.history.ui.port = 18080 #default 18080</span></span><br><span class="line">    <span class="comment"># spark.history.fs.cleaner.enabled    = true # default is false</span></span><br><span class="line">    <span class="comment"># spark.history.fs.cleaner.interval    1d</span></span><br><span class="line">    <span class="comment"># spark.history.fs.cleaner.maxAge    7d</span></span><br><span class="line">    <span class="comment"># spark.history.fs.numReplayThreads    # Default = 25% of available cores</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># spark.shuffle.service.enabled = true</span></span><br><span class="line"></span><br><span class="line">    spark.driver.extraJavaOptions = <span class="string">"-Duser.timezone=Australia/Sydney -Dlog4j.configuration=log4j2.xml"</span></span><br><span class="line"></span><br><span class="line">    spark.executor.extraJavaOptions = <span class="string">"-Duser.timezone=Australia/Sydney -Dlog4j.configuration=log4j2.xml"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#spark.dynamicAllocation.enabled = true</span></span><br><span class="line">    <span class="comment">#spark.default.parallelism = 200</span></span><br><span class="line">    <span class="comment">#passthrough &#123;</span></span><br><span class="line">      <span class="comment">#spark.sql.parquet.output.committer.class = org.apache.spark.sql.parquet.DirectParquetOutputCommitter</span></span><br><span class="line">      <span class="comment">#mapreduce.fileoutputcommitter.marksuccessfuljobs = false</span></span><br><span class="line">    <span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line">    spark.driver.port=6000</span><br><span class="line">    spark.blockManager.port=6100</span><br><span class="line">    spark.port.maxRetries=16 <span class="comment">#default 16</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># in case spark distribution should be accessed from HDFS (as opposed to being installed on every mesos slave)</span></span><br><span class="line">    <span class="comment"># spark.executor.uri = "hdfs://namenode:8020/apps/spark/spark.tgz"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># uris of jars to be loaded into the classpath for this context. Uris is a string list, or a string separated by commas ','</span></span><br><span class="line">    <span class="comment"># dependent-jar-uris = ["file:///some/path/present/in/each/mesos/slave/somepackage.jar"]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,</span></span><br><span class="line">    <span class="comment"># such as hadoop connection settings that don't use the "spark." prefix</span></span><br><span class="line">    passthrough &#123;</span><br><span class="line">      <span class="comment">#es.nodes = "192.1.1.1"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This needs to match SPARK_HOME for cluster SparkContexts to be created successfully</span></span><br><span class="line">  home = <span class="string">"/spark"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">deploy &#123;</span><br><span class="line">  manager-start-cmd = <span class="string">"app/manager_start.sh"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">spray.can.server &#123;</span><br><span class="line">  <span class="comment"># uncomment the next lines for making this an HTTPS example</span></span><br><span class="line">  <span class="comment"># Converting PEM-format keys to JKS format:</span></span><br><span class="line">  <span class="comment"># https://docs.oracle.com/cd/E35976_01/server.740/es_admin/src/tadm_ssl_convert_pem_to_jks.html</span></span><br><span class="line">  <span class="comment"># ssl-encryption = on</span></span><br><span class="line">  <span class="comment"># path to keystore</span></span><br><span class="line">  <span class="comment"># keystore = "/some/path/sjs.jks"</span></span><br><span class="line">  <span class="comment"># keystorePW = "changeit"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># see http://docs.oracle.com/javase/7/docs/technotes/guides/security/StandardNames.html#SSLContext for more examples</span></span><br><span class="line">  <span class="comment"># typical are either SSL or TLS</span></span><br><span class="line">  encryptionType = <span class="string">"SSL"</span></span><br><span class="line">  keystoreType = <span class="string">"JKS"</span></span><br><span class="line">  <span class="comment"># key manager factory provider</span></span><br><span class="line">  provider = <span class="string">"SunX509"</span></span><br><span class="line">  <span class="comment"># ssl engine provider protocols</span></span><br><span class="line">  enabledProtocols = [<span class="string">"SSLv3"</span>, <span class="string">"TLSv1"</span>]</span><br><span class="line"></span><br><span class="line">  idle-timeout = 600 s <span class="comment"># default is 60s</span></span><br><span class="line">  request-timeout = 300 s <span class="comment"># default is 40s</span></span><br><span class="line"></span><br><span class="line">  pipelining-limit = 2 <span class="comment"># for maximum performance (prevents StopReading / ResumeReading messages to the IOBridge)</span></span><br><span class="line">  <span class="comment"># Needed for HTTP/1.0 requests with missing Host headers</span></span><br><span class="line">  default-host-header = <span class="string">"spray.io:8765"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Increase this in order to upload bigger job jars</span></span><br><span class="line">  parsing.max-content-length = 2g</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that you can use this file to define settings not only for job server,</span></span><br><span class="line"><span class="comment"># but for your Spark jobs as well.  Spark job configuration merges with this configuration file as defaults.</span></span><br></pre></td></tr></table></figure>
<h3 id="2-Build-docker-image"><a href="#2-Build-docker-image" class="headerlink" title="2. Build docker image"></a>2. Build docker image</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; sbt</span><br><span class="line">&gt; docker</span><br></pre></td></tr></table></figure>
<h3 id="3-Spark-local-standalone-mode"><a href="#3-Spark-local-standalone-mode" class="headerlink" title="3. Spark local-standalone mode"></a>3. Spark local-standalone mode</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d -p 8090:8090 -p 6000-6015:6000-6015 -p 6100-6115:6100-6115 &lt;ORG&gt;/spark-jobserver</span><br><span class="line"></span><br><span class="line">// curl -X DELETE <span class="string">"127.0.0.1:8090/contexts/test-context"</span></span><br><span class="line"></span><br><span class="line">curl -X POST <span class="string">"127.0.0.1:8090/contexts/test-context"</span></span><br><span class="line"></span><br><span class="line">curl -X GET <span class="string">"127.0.0.1:8090/contexts"</span></span><br><span class="line"></span><br><span class="line">// Get a jar package <span class="keyword">for</span> testing</span><br><span class="line"></span><br><span class="line">// Upload a <span class="built_in">test</span> jar</span><br><span class="line">curl --data-binary @job-server-tests_2.11-0.8.1.jar 127.0.0.1:8090/jars/<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">// Using sync and get result immediately</span><br><span class="line">curl -d <span class="string">"input.string = a b c a b see"</span> <span class="string">"&lt;Master IP&gt;:8090/jobs?appName=test&amp;classPath=spark.jobserver.WordCountExample&amp;context=test-context&amp;sync=true"</span></span><br><span class="line"></span><br><span class="line">curl -d <span class="string">"sql = \"select * from addresses limit 10\""</span> <span class="string">'&lt;Master IP&gt;:8090/jobs?appName=test&amp;classPath=spark.jobserver.SqlTestJob&amp;context=test-context&amp;sync=true'</span></span><br></pre></td></tr></table></figure>
<h3 id="4-Spark-standalone-mode"><a href="#4-Spark-standalone-mode" class="headerlink" title="4. Spark standalone mode"></a>4. Spark standalone mode</h3><ul>
<li>Start Spark job-server docker</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d -p 8090:8090 -p 4040-4055:4040-4055 -p 6000-6015:6000-6015 -p 6100-6115:6100-6115 -e SPARK_MASTER=spark://&lt;Master IP&gt;:7077 &lt;ORG&gt;/spark-jobserver</span><br><span class="line"></span><br><span class="line">// Get docker ip address</span><br><span class="line">sudo docker inspect 1afe8695c688 // (jobserver container id 172.17.0.2)</span><br></pre></td></tr></table></figure>
<ul>
<li>Running Spark-SQL Query </li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">curl -i -d <span class="string">""</span> <span class="string">'localhost:8090/contexts/test-context?spark.scheduler.mode=FAIR&amp;spark.driver.host=&lt;Master IP&gt;&amp;spark.driver.bindAddress=172.17.0.2&amp;spark.driver.port=6000&amp;spark.blockManager.port=6100&amp;spark.ui.port=4050&amp;spark.sql.crossJoin.enabled=true&amp;num-cpu-cores=45&amp;memory-per-node=60G'</span></span><br><span class="line"></span><br><span class="line">curl -X GET <span class="string">"127.0.0.1:8090/contexts"</span></span><br><span class="line"></span><br><span class="line">// curl -X DELETE <span class="string">"127.0.0.1:8090/contexts/test-context"</span></span><br><span class="line"></span><br><span class="line">// Download job-server-tests_2.11/0.8.1/job-server-tests_2.11-0.8.1.jar</span><br><span class="line"></span><br><span class="line">// Upload a <span class="built_in">test</span> jar</span><br><span class="line">curl --data-binary @job-server-tests_2.11-0.8.1.jar 127.0.0.1:8090/jars/<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">// Using sync and get result immediately</span><br><span class="line">curl -d <span class="string">"input.string = a b c a b see"</span> <span class="string">"&lt;Master IP&gt;:8090/jobs?appName=test&amp;classPath=spark.jobserver.WordCountExample&amp;context=test-context&amp;sync=true"</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Running Spark-SQL Query which enables Spark-SQL and Hive support</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// Create a SparkSession context <span class="built_in">which</span> enables Spark-SQL and Hive support</span><br><span class="line">curl -i -d <span class="string">""</span> <span class="string">'http://localhost:8090/contexts/sql-context?spark.scheduler.mode=FAIR&amp;spark.driver.host=&lt;Master IP&gt;&amp;spark.driver.bindAddress=172.17.0.2&amp;spark.driver.port=6000&amp;spark.blockManager.port=6100&amp;spark.ui.port=4050&amp;spark.sql.crossJoin.enabled=true&amp;num-cpu-cores=45&amp;memory-per-node=60G&amp;context-factory=spark.jobserver.context.SessionContextFactory'</span></span><br><span class="line"></span><br><span class="line">curl -d <span class="string">"sql = \"select * from addresses limit 10\""</span> <span class="string">'&lt;Master IP&gt;:8090/jobs?appName=test&amp;classPath=spark.jobserver.SqlTestJob&amp;context=sql-context&amp;sync=true'</span></span><br></pre></td></tr></table></figure>
<h3 id="Additional-parameters-https-spark-apache-org-docs-latest-job-scheduling-html"><a href="#Additional-parameters-https-spark-apache-org-docs-latest-job-scheduling-html" class="headerlink" title="Additional parameters - https://spark.apache.org/docs/latest/job-scheduling.html"></a>Additional parameters - <a href="https://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/job-scheduling.html</a></h3><ul>
<li>Setting Fair mode with pools’ properties</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">// Assuming sc is your SparkContext variable</span><br><span class="line">sc.setLocalProperty(<span class="string">"spark.scheduler.pool"</span>, <span class="string">"production"</span>)</span><br><span class="line"></span><br><span class="line">spark.scheduler.pool = <span class="string">"production"</span></span><br><span class="line">spark.scheduler.allocation.file = <span class="string">"<span class="variable">$SPARK_HOME_PATH</span>/conf/fairscheduler.xml"</span></span><br><span class="line"></span><br><span class="line">&lt;?xml version=<span class="string">"1.0"</span>?&gt;</span><br><span class="line">&lt;allocations&gt;</span><br><span class="line">  &lt;pool name=<span class="string">"production"</span>&gt;</span><br><span class="line">    &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt;</span><br><span class="line">    &lt;weight&gt;1&lt;/weight&gt;</span><br><span class="line">    &lt;minShare&gt;2&lt;/minShare&gt;</span><br><span class="line">  &lt;/pool&gt;</span><br><span class="line">  &lt;pool name=<span class="string">"test"</span>&gt;</span><br><span class="line">    &lt;schedulingMode&gt;FIFO&lt;/schedulingMode&gt;</span><br><span class="line">    &lt;weight&gt;2&lt;/weight&gt;</span><br><span class="line">    &lt;minShare&gt;3&lt;/minShare&gt;</span><br><span class="line">  &lt;/pool&gt;</span><br><span class="line">&lt;/allocations&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>max-jobs-per-context</li>
</ul>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// The default is 8 <span class="built_in">which</span> probably translates to about 6 concurrent <span class="built_in">jobs</span>.</span><br><span class="line">spark.jobserver.max-jobs-per-context=10</span><br></pre></td></tr></table></figure>
<h2 id="VI-Network-Configuration"><a href="#VI-Network-Configuration" class="headerlink" title="VI. Network Configuration"></a>VI. Network Configuration</h2><p>If we need to deploy the bigdata platform to a network stricted environment.</p>
<p><a href="https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azkic_t_confignetwork.htm" target="_blank" rel="noopener">Network configuration for Spark</a></p>
<p><img src="https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/images/spark-ports.gif" alt="alt text"></p>
<h3 id="Exposed-ports"><a href="#Exposed-ports" class="headerlink" title="Exposed ports"></a>Exposed ports</h3><ul>
<li><p>Application ports: </p>
<ul>
<li>443 (HTTPS)</li>
<li>22 (SSH)</li>
<li>3306 (MySQL)</li>
</ul>
</li>
<li><p>Spark ports: <a href="https://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">Configuration</a></p>
<ul>
<li>4040-4055 (spark.ui.port)</li>
<li>7077 (SPARK_MASTER_PORT)</li>
<li>8080(spark.master.ui.port) </li>
<li>8081 (spark.worker.ui.port)</li>
<li>18080(spark.history.ui.port) </li>
<li>6000-6015(spark.driver.port) </li>
<li>6100-6115(spark.blockManager.port / [spark.blockManager.port (no need)])</li>
</ul>
</li>
<li><p>Spark Job-server: <a href="https://github.com/spark-jobserver/spark-jobserver" target="_blank" rel="noopener">spark-jobserver github</a></p>
<ul>
<li>8090 (job-server UI) </li>
</ul>
</li>
<li><p>Hadoop: <a href="https://hadoop.apache.org/docs/r2.7.5/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" target="_blank" rel="noopener">hdfs-default.xml</a></p>
<ul>
<li>50010 dfs.datanode.address</li>
<li>50020 dfs.datanode.ipc.address</li>
<li>50070 dfs.namenode.http-address</li>
<li><p>50075 dfs.datanode.http.address</p>
</li>
<li><p>50470 dfs.namenode.https-address</p>
</li>
<li><p>50475 dfs.datanode.https.address</p>
</li>
<li><p>50090 dfs.namenode.secondary.http-address</p>
</li>
<li>50091 dfs.namenode.secondary.https-address</li>
<li>50100 dfs.namenode.backup.address</li>
<li>50105 dfs.namenode.backup.http-address</li>
<li>9000 fs.default.name for hdfs file system in /opt/hadoop/etc/hadoop/core-site.xml</li>
<li>8020 could be used as fs.default.name</li>
</ul>
</li>
<li><p>Mapred ports: <a href="https://hadoop.apache.org/docs/r2.7.5/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml" target="_blank" rel="noopener">mapred-default.xml</a></p>
<ul>
<li>10020 mapreduce.jobhistory.address</li>
<li>19888 mapreduce.jobhistory.webapp.address</li>
<li><p>10033 mapreduce.jobhistory.admin.address</p>
</li>
<li><p>50030 mapreduce.jobtracker.http.address</p>
</li>
<li>50060 mapreduce.tasktracker.http.address</li>
</ul>
</li>
<li><p>Yarn ports: <a href="https://hadoop.apache.org/docs/r2.7.5/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" target="_blank" rel="noopener">yarn-default.xml</a></p>
<ul>
<li>8030 yarn.resourcemanager.scheduler.address</li>
<li>8031 yarn.resourcemanager.resource-tracker.address</li>
<li>8032 yarn.resourcemanager.address</li>
<li>8033 yarn.resourcemanager.admin.address</li>
<li>8040 yarn.nodemanager.localizer.address</li>
<li>8042 yarn.nodemanager.webapp.address</li>
<li>8088 yarn.resourcemanager.webapp.address</li>
<li>8090 yarn.resourcemanager.webapp.https.address</li>
</ul>
</li>
<li><p>Other ports: </p>
<ul>
<li>49707 (tcp/udp)</li>
<li>2122 (tcp/udp)</li>
</ul>
</li>
</ul>
<h3 id="Add-all-nodes-domain-names-to-etc-hosts"><a href="#Add-all-nodes-domain-names-to-etc-hosts" class="headerlink" title="Add all nodes domain names to /etc/hosts"></a>Add all nodes domain names to /etc/hosts</h3><h3 id="Checking-LISTEN-ports"><a href="#Checking-LISTEN-ports" class="headerlink" title="Checking LISTEN ports"></a>Checking LISTEN ports</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ netstat -aunpt | grep java</span><br><span class="line"></span><br><span class="line">tcp        0      0 0.0.0.0:50010           0.0.0.0:*               LISTEN      9773/java</span><br><span class="line">tcp        0      0 0.0.0.0:50075           0.0.0.0:*               LISTEN      9773/java</span><br><span class="line">tcp        0      0 127.0.0.1:40893         0.0.0.0:*               LISTEN      9773/java</span><br><span class="line">tcp        0      0 0.0.0.0:50020           0.0.0.0:*               LISTEN      9773/java</span><br><span class="line">tcp        0      0 &lt;localIP&gt;:&lt;randomPort&gt;  &lt;namenodePort&gt;:9000     ESTABLISHED 9773/java</span><br><span class="line"></span><br><span class="line">$ ps aux | grep 9773</span><br><span class="line">&lt;username&gt;+   9773  0.0  0.5 2913968 382280 ?      Sl   Feb07  37:52 /usr/java/default/bin/java -Dproc_datanode -Xmx1000m -Djava.net.preferIPv4Stack=<span class="literal">true</span> -...Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line">&lt;username&gt;+  72466  0.0  0.0 112652   972 pts/0    S+   15:58   0:00 grep --color=auto 9773</span><br></pre></td></tr></table></figure>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Big-data/" rel="tag"># Big data</a>
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
            <a href="/tags/Hadoop/" rel="tag"># Hadoop</a>
          
            <a href="/tags/Spark-Job-server/" rel="tag"># Spark Job-server</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/03/SparkOperations/" rel="next" title="Spark Operations">
                <i class="fa fa-chevron-left"></i> Spark Operations
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/02/21/DockerCommandsCheatsheet/" rel="prev" title="Docker Commands Cheatsheet">
                Docker Commands Cheatsheet <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Cubean Liu" />
          <p class="site-author-name" itemprop="name">Cubean Liu</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">tags</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:cubean.liu@gmail.com" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/cubean/?locale=en_US" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      LinkedIn
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/cubean" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/CubeanLiu" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                    
                      Twitter
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.facebook.com/cubean.liu" target="_blank" title="FB Page">
                  
                    <i class="fa fa-fw fa-facebook"></i>
                  
                    
                      FB Page
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.cubeanliu.com/" title="CubeanBlog" target="_blank">CubeanBlog</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#I-Version-and-Environment"><span class="nav-number">1.</span> <span class="nav-text">I. Version and Environment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Spark-Yarn-HDFS"><span class="nav-number">1.1.</span> <span class="nav-text">1. Spark, Yarn, HDFS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Spark-Job-Server"><span class="nav-number">1.2.</span> <span class="nav-text">2. Spark Job Server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-OS-in-AWS"><span class="nav-number">1.3.</span> <span class="nav-text">3. OS in AWS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference-Installation"><span class="nav-number">1.4.</span> <span class="nav-text">Reference Installation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#II-Building-YARN-HDFS-cluster-servers"><span class="nav-number">2.</span> <span class="nav-text">II. Building YARN-HDFS cluster servers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Mount-2nd-EBS-disk-to-HDFS-folder"><span class="nav-number">2.1.</span> <span class="nav-text">1. Mount 2nd EBS disk to HDFS folder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Install-Hadoop"><span class="nav-number">2.2.</span> <span class="nav-text">2. Install Hadoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-bash-environment-changes"><span class="nav-number">2.3.</span> <span class="nav-text">3. bash environment changes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Configuring-Hadoop"><span class="nav-number">2.4.</span> <span class="nav-text">4. Configuring Hadoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Running-Hadoop-YARN"><span class="nav-number">2.5.</span> <span class="nav-text">5. Running Hadoop-YARN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#III-Building-Spark-cluster-servers"><span class="nav-number">3.</span> <span class="nav-text">III. Building Spark cluster servers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-In-all-nodes-need-to-add-domain-names-in-etc-hosts"><span class="nav-number">3.1.</span> <span class="nav-text">1. In all nodes, need to add domain names in /etc/hosts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Setup-master-node"><span class="nav-number">3.2.</span> <span class="nav-text">2. Setup master node</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Start-Spark-cluster"><span class="nav-number">3.3.</span> <span class="nav-text">3. Start Spark cluster</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#IV-Install-Docker"><span class="nav-number">4.</span> <span class="nav-text">IV. Install Docker</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Mount-2nd-EBS-disk-to-docker-folder"><span class="nav-number">4.1.</span> <span class="nav-text">1. Mount 2nd EBS disk to docker folder:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Install-Docker-community-version"><span class="nav-number">4.2.</span> <span class="nav-text">2. Install Docker community version</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Optional-Install-Ansible"><span class="nav-number">4.3.</span> <span class="nav-text">3. [Optional] Install Ansible</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#V-Run-spark-job-server"><span class="nav-number">5.</span> <span class="nav-text">V. Run spark-job-server</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Spark-job-server-docker-config"><span class="nav-number">5.1.</span> <span class="nav-text">1. Spark job-server docker config</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Build-docker-image"><span class="nav-number">5.2.</span> <span class="nav-text">2. Build docker image</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Spark-local-standalone-mode"><span class="nav-number">5.3.</span> <span class="nav-text">3. Spark local-standalone mode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Spark-standalone-mode"><span class="nav-number">5.4.</span> <span class="nav-text">4. Spark standalone mode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Additional-parameters-https-spark-apache-org-docs-latest-job-scheduling-html"><span class="nav-number">5.5.</span> <span class="nav-text">Additional parameters - https://spark.apache.org/docs/latest/job-scheduling.html</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VI-Network-Configuration"><span class="nav-number">6.</span> <span class="nav-text">VI. Network Configuration</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exposed-ports"><span class="nav-number">6.1.</span> <span class="nav-text">Exposed ports</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Add-all-nodes-domain-names-to-etc-hosts"><span class="nav-number">6.2.</span> <span class="nav-text">Add all nodes domain names to /etc/hosts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Checking-LISTEN-ports"><span class="nav-number">6.3.</span> <span class="nav-text">Checking LISTEN ports</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cubean Liu</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://cubean-blog.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://cubeanliu.com/2018/02/20/ProductiveSparkCluster/';
          this.page.identifier = '2018/02/20/ProductiveSparkCluster/';
          this.page.title = 'Productive Spark Cluster in YARN-Client Mode';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://cubean-blog.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  








  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("DV78LWwAigdG28Tgf5PkGis3-MdYXbMMI", "p1FXgnm5B74s5PG7zf0V3t9Y");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
